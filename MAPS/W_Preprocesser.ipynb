{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skikit may only be avaiable in my non-GPU environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import xarray as xr\n",
    "import dask\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data sample.  This one has already been spliced by lat/lon and Vertical Velocity pulled out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/DFS-L/DATA/pritchard/gmooers/Workflow/MAPS/SPCAM/Small_Sample/Data_Points/One_Day_Merged_Data.nc'\n",
    "real_ds = xr.open_dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_velocity = real_ds['CRM_W'].values\n",
    "w_velocity = np.squeeze(w_velocity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array is currently set up as:\n",
    "\n",
    "(location, time, level, crm_x)\n",
    "\n",
    "where location is the 109 lat/lon points that pass the filtering test for probable deep converction in the afternoon (lst time)\n",
    "\n",
    "Time is in 15 minute intervals\n",
    "\n",
    "30 vertical levels divide up the atmosphere\n",
    "\n",
    "128 CRMs in the x direction per GCM grid cell\n",
    "\n",
    "(109, 96, 30, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to reshape array so time dimension can be shuffled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = len(w_velocity[0])\n",
    "coords = len(w_velocity)\n",
    "lev = len(w_velocity[0][0])\n",
    "crm_x = len(w_velocity[0][0][0])\n",
    "w_new = np.zeros(shape =(t, coords, lev, crm_x))\n",
    "w_new[:,:,:,:] = np.nan\n",
    "for i in range(len(w_velocity)):\n",
    "    for j in range(len(w_velocity[i])):\n",
    "        w_new[j, i, :, :] = w_velocity[i,j,:,:]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must check to see if array has nan values within it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(w_new).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Morphology Tests, e.g. feeding in low resolution image snap shots, I do not want a diurnal cycle, so I will shuffle by time:\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/random/shuffle\n",
    "\n",
    "I seem to need to use a tensorflow built in function to do this on an array more than two dimensions....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_shuffled = tf.random.shuffle(w_new, seed=None, name=None)\n",
    "sess = tf.InteractiveSession()\n",
    "w_numpy = w_shuffled.eval()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to split data into training and test sections:\n",
    "\n",
    "Will do an 80/20 split for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_train = w_numpy[:int(4*len(w_numpy)/5),:,:,:]\n",
    "w_test = w_numpy[int(4*len(w_numpy)/5):,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must scale all array values to between 0 and 1\n",
    "\n",
    "Seems standardization not normalization is apropriate\n",
    "- both training and validation data\n",
    "\n",
    "https://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization\n",
    "\n",
    "Skikit learn has built in fuctions to do this:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "\n",
    "Actually, disregard above - code only works for 2D array, I will have to do this manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.763671875\n",
      "-8.849842071533203\n"
     ]
    }
   ],
   "source": [
    "print(np.max(w_numpy))\n",
    "print(np.min(w_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1:\n",
    "\n",
    "Assign z scores centered around $\\mu$ of 0 and $\\sigma$ = 1\n",
    "Standardization:\n",
    "\n",
    "$X^` = \\frac{x - \\mu}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_train = (w_train - w_train.mean(axis=(2,3),keepdims=1)) / w_train.std(axis=(2,3),keepdims=1)\n",
    "rescaled_test = (w_test - w_test.mean(axis=(2,3),keepdims=1)) / w_test.std(axis=(2,3),keepdims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.640037919268302\n",
      "-21.487122774948084\n",
      "31.45649386707715\n",
      "-19.78183682514497\n"
     ]
    }
   ],
   "source": [
    "print(np.max(rescaled_train))\n",
    "print(np.min(rescaled_train))\n",
    "print(np.max(rescaled_test))\n",
    "print(np.min(rescaled_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2:\n",
    "\n",
    "Normalization: Scale each value in arrray between 0 to 1.  This seems to be method of choice in most \"image\" problems where they divide by 255. to get pixels between 0 and 1, so I will defer to it for now?\n",
    "\n",
    "$X^` = \\frac{x - min(x)}{max(x)-min(x)}$\n",
    "\n",
    "The built in interpolation function will allow this to easily be done in a line of code\n",
    "\n",
    "https://stackoverflow.com/questions/36000843/scale-numpy-array-to-certain-range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_train = np.interp(w_train, (w_train.min(), w_train.max()), (0, +1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_test = np.interp(w_test, (w_train.min(), w_train.max()), (0, +1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "0.848476042326487\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(rescaled_train))\n",
    "print(np.min(rescaled_train))\n",
    "print(np.max(rescaled_test))\n",
    "print(np.min(rescaled_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the arrays into a number of \"Low Res Images\" - e.g. 30x128 arrays that can be fed into the VAE as snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = np.zeros(shape=(int(4*t/5)*coords, lev, crm_x))\n",
    "final_train[:,:,:] = np.nan\n",
    "count = 0\n",
    "\n",
    "for i in range(len(rescaled_train)):\n",
    "    for j in range(len(rescaled_train[i])):\n",
    "        final_train[count, :, :] = rescaled_train[i,j,:,:]\n",
    "        count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = np.zeros(shape=((t-int(4*t/5))*coords, lev, crm_x))\n",
    "final_test[:,:,:] = np.nan\n",
    "count = 0\n",
    "\n",
    "for i in range(len(rescaled_test)):\n",
    "    for j in range(len(rescaled_test[i])):\n",
    "        final_test[count, :, :] = rescaled_test[i,j,:,:]\n",
    "        count = count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save these training and test datasets to standard Preprocessed Folder for use in the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/fast/gmooers/Preprocessed_Data/W_Trial/W_Training.npy', final_train)\n",
    "np.save('/fast/gmooers/Preprocessed_Data/W_Trial/W_Test.npy', final_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
